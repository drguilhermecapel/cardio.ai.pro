"""
Comprehensive tests for Hybrid CNN-BiLSTM-Transformer Architecture
Tests deep learning components and integrated dataset functionality
"""

import pytest
import torch
import torch.nn as nn
import numpy as np
from unittest.mock import Mock, patch, MagicMock
import asyncio
from typing import Dict, Any, Tuple

from app.ml.hybrid_architecture import (
    HybridECGModel, ModelConfig, FrequencyChannelAttention,
    DenseBlock, TransitionLayer, EnsembleVoting
)
from app.ml.training_pipeline import TrainingPipeline, TrainingConfig, ECGMultimodalDataset


class TestHybridArchitecture:
    """Test suite for Hybrid CNN-BiLSTM-Transformer Architecture"""
    
    @pytest.fixture
    def model_config(self):
        """Create model configuration for testing"""
        return ModelConfig(
            num_classes=71,
            input_channels=12,
            sequence_length=5000,
            cnn_channels=[64, 128, 256],
            lstm_hidden_size=256,
            lstm_num_layers=2,
            transformer_d_model=512,
            transformer_nhead=8,
            transformer_num_layers=6,
            dropout=0.1,
            use_attention=True,
            ensemble_weights=[0.3, 0.3, 0.4]
        )
    
    @pytest.fixture
    def sample_ecg_batch(self):
        """Create sample ECG batch for testing"""
        batch_size = 4
        channels = 12
        sequence_length = 5000
        return torch.randn(batch_size, channels, sequence_length)
    
    @pytest.fixture
    def sample_labels(self):
        """Create sample labels for testing"""
        batch_size = 4
        num_classes = 71
        return torch.randint(0, 2, (batch_size, num_classes)).float()
    
    def test_model_config_initialization(self, model_config):
        """Test model configuration initialization"""
        assert model_config.num_classes == 71
        assert model_config.input_channels == 12
        assert model_config.sequence_length == 5000
        assert len(model_config.cnn_channels) == 3
        assert model_config.lstm_hidden_size == 256
        assert model_config.transformer_d_model == 512
        assert model_config.transformer_nhead == 8
        assert len(model_config.ensemble_weights) == 3
        assert abs(sum(model_config.ensemble_weights) - 1.0) < 1e-6
    
    def test_frequency_channel_attention(self):
        """Test Frequency Channel Attention mechanism"""
        in_channels = 128
        attention = FrequencyChannelAttention(in_channels)
        
        x = torch.randn(2, in_channels, 32, 32)
        output = attention(x)
        
        assert output.shape == x.shape
        
        assert not torch.equal(output, x)  # Should be different from input
        
        attention_weights = attention.channel_attention(attention.avg_pool(x))
        assert attention_weights.shape == (2, in_channels, 1, 1)
        assert torch.all(attention_weights >= 0)  # Should be non-negative after sigmoid
        assert torch.all(attention_weights <= 1)
    
    def test_dense_block(self):
        """Test DenseNet Dense Block"""
        in_channels = 64
        growth_rate = 32
        num_layers = 4
        
        dense_block = DenseBlock(in_channels, growth_rate, num_layers)
        
        x = torch.randn(2, in_channels, 64, 64)
        output = dense_block(x)
        
        expected_out_channels = in_channels + growth_rate * num_layers
        assert output.shape[1] == expected_out_channels
        assert output.shape[0] == x.shape[0]  # Batch size unchanged
        assert output.shape[2] == x.shape[2]  # Height unchanged
        assert output.shape[3] == x.shape[3]  # Width unchanged
    
    def test_transition_layer(self):
        """Test DenseNet Transition Layer"""
        in_channels = 128
        out_channels = 64
        
        transition = TransitionLayer(in_channels, out_channels)
        
        x = torch.randn(2, in_channels, 32, 32)
        output = transition(x)
        
        assert output.shape[1] == out_channels
        assert output.shape[0] == x.shape[0]  # Batch size unchanged
        assert output.shape[2] == x.shape[2] // 2  # Height halved
        assert output.shape[3] == x.shape[3] // 2  # Width halved
    
    def test_hybrid_model_initialization(self, model_config):
        """Test hybrid model initialization"""
        model = HybridECGModel(model_config)
        
        assert hasattr(model, 'cnn_backbone')
        assert hasattr(model, 'lstm_branch')
        assert hasattr(model, 'transformer_branch')
        assert hasattr(model, 'ensemble_voting')
        
        assert hasattr(model.cnn_backbone, 'conv1')
        assert hasattr(model.cnn_backbone, 'dense_blocks')
        assert hasattr(model.cnn_backbone, 'transition_layers')
        
        assert isinstance(model.lstm_branch, nn.LSTM)
        assert model.lstm_branch.input_size == model_config.cnn_channels[-1]
        assert model.lstm_branch.hidden_size == model_config.lstm_hidden_size
        assert model.lstm_branch.num_layers == model_config.lstm_num_layers
        assert model.lstm_branch.bidirectional == True
        
        assert hasattr(model.transformer_branch, 'transformer_encoder')
        assert hasattr(model.transformer_branch, 'positional_encoding')
        
        assert isinstance(model.ensemble_voting, EnsembleVoting)
    
    def test_hybrid_model_forward_pass(self, model_config, sample_ecg_batch):
        """Test hybrid model forward pass"""
        model = HybridECGModel(model_config)
        model.eval()
        
        with torch.no_grad():
            output = model(sample_ecg_batch)
        
        assert isinstance(output, dict)
        assert 'final_logits' in output
        assert 'cnn_logits' in output
        assert 'lstm_logits' in output
        assert 'transformer_logits' in output
        assert 'features' in output
        
        batch_size = sample_ecg_batch.shape[0]
        num_classes = model_config.num_classes
        
        assert output['final_logits'].shape == (batch_size, num_classes)
        assert output['cnn_logits'].shape == (batch_size, num_classes)
        assert output['lstm_logits'].shape == (batch_size, num_classes)
        assert output['transformer_logits'].shape == (batch_size, num_classes)
        
        assert 'cnn' in output['features']
        assert 'lstm' in output['features']
        assert 'transformer' in output['features']
        
        assert len(output['features']['cnn'].shape) >= 2
        assert len(output['features']['lstm'].shape) >= 2
        assert len(output['features']['transformer'].shape) >= 2
    
    def test_ensemble_voting(self):
        """Test ensemble voting mechanism"""
        num_classes = 71
        batch_size = 4
        weights = [0.3, 0.3, 0.4]
        
        ensemble = EnsembleVoting(weights)
        
        cnn_logits = torch.randn(batch_size, num_classes)
        lstm_logits = torch.randn(batch_size, num_classes)
        transformer_logits = torch.randn(batch_size, num_classes)
        
        final_logits = ensemble(cnn_logits, lstm_logits, transformer_logits)
        
        assert final_logits.shape == (batch_size, num_classes)
        
        expected = (weights[0] * cnn_logits + 
                   weights[1] * lstm_logits + 
                   weights[2] * transformer_logits)
        
        assert torch.allclose(final_logits, expected, atol=1e-6)
    
    def test_model_multimodal_fusion(self, model_config):
        """Test multimodal fusion capabilities"""
        model = HybridECGModel(model_config)
        model.eval()
        
        batch_size = 2
        
        signal_1d = torch.randn(batch_size, 12, 5000)
        
        spectrogram_2d = torch.randn(batch_size, 12, 128, 128)
        
        with torch.no_grad():
            output_1d = model(signal_1d)
            
            assert 'final_logits' in output_1d
            assert output_1d['final_logits'].shape[0] == batch_size
    
    def test_model_performance_requirements(self, model_config, sample_ecg_batch, sample_labels):
        """Test model performance requirements"""
        model = HybridECGModel(model_config)
        
        import time
        
        model.eval()
        with torch.no_grad():
            start_time = time.time()
            output = model(sample_ecg_batch)
            end_time = time.time()
        
        processing_time = (end_time - start_time) / sample_ecg_batch.shape[0]  # Per sample
        
        assert processing_time < 0.1, f"Processing time {processing_time:.3f}s exceeds 100ms requirement"
        
        assert torch.all(torch.isfinite(output['final_logits']))  # No NaN or Inf
        assert not torch.all(output['final_logits'] == 0)  # Not all zeros
    
    def test_model_gradient_flow(self, model_config, sample_ecg_batch, sample_labels):
        """Test gradient flow through the model"""
        model = HybridECGModel(model_config)
        criterion = nn.BCEWithLogitsLoss()
        
        output = model(sample_ecg_batch)
        loss = criterion(output['final_logits'], sample_labels)
        
        loss.backward()
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                assert param.grad is not None, f"No gradient for parameter {name}"
                assert torch.any(param.grad != 0), f"Zero gradient for parameter {name}"
    
    def test_model_memory_efficiency(self, model_config, sample_ecg_batch):
        """Test model memory efficiency"""
        model = HybridECGModel(model_config)
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        assert total_params < 50_000_000, f"Model too large: {total_params} parameters"
        assert trainable_params == total_params  # All parameters should be trainable
        
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        model.eval()
        with torch.no_grad():
            output = model(sample_ecg_batch)
        
        assert 'final_logits' in output


class TestTrainingPipeline:
    """Test suite for Training Pipeline"""
    
    @pytest.fixture
    def training_config(self):
        """Create training configuration for testing"""
        return TrainingConfig(
            model_config=ModelConfig(
                num_classes=71,
                input_channels=12,
                sequence_length=5000
            ),
            batch_size=8,
            learning_rate=1e-4,
            num_epochs=2,  # Small for testing
            weight_decay=1e-5,
            scheduler_step_size=10,
            scheduler_gamma=0.1,
            early_stopping_patience=5,
            curriculum_learning=True,
            curriculum_epochs=[1, 2],
            curriculum_difficulties=[0.5, 1.0],
            use_wandb=False,  # Disable for testing
            wandb_project="test_project",
            save_checkpoints=True,
            checkpoint_dir="/tmp/test_checkpoints",
            validate_every=1,
            log_every=10
        )
    
    @pytest.fixture
    def sample_dataset(self):
        """Create sample dataset for testing"""
        num_samples = 32
        signals = torch.randn(num_samples, 12, 5000)
        labels = torch.randint(0, 2, (num_samples, 71)).float()
        
        dataset = ECGMultimodalDataset(
            signals=signals,
            labels=labels,
            sampling_rate=500,
            augment=False  # Disable augmentation for testing
        )
        
        return dataset
    
    def test_training_config_initialization(self, training_config):
        """Test training configuration initialization"""
        assert training_config.batch_size == 8
        assert training_config.learning_rate == 1e-4
        assert training_config.num_epochs == 2
        assert training_config.curriculum_learning == True
        assert len(training_config.curriculum_epochs) == 2
        assert len(training_config.curriculum_difficulties) == 2
        assert training_config.use_wandb == False
    
    def test_ecg_multimodal_dataset(self, sample_dataset):
        """Test ECG multimodal dataset"""
        assert len(sample_dataset) == 32
        
        sample = sample_dataset[0]
        assert isinstance(sample, dict)
        assert 'signal' in sample
        assert 'labels' in sample
        assert 'metadata' in sample
        
        assert sample['signal'].shape == (12, 5000)
        assert sample['labels'].shape == (71,)
        
        assert 'sampling_rate' in sample['metadata']
        assert sample['metadata']['sampling_rate'] == 500
    
    def test_training_pipeline_initialization(self, training_config):
        """Test training pipeline initialization"""
        pipeline = TrainingPipeline(training_config)
        
        assert hasattr(pipeline, 'config')
        assert hasattr(pipeline, 'model')
        assert hasattr(pipeline, 'optimizer')
        assert hasattr(pipeline, 'scheduler')
        assert hasattr(pipeline, 'criterion')
        
        assert isinstance(pipeline.model, HybridECGModel)
        
        assert hasattr(pipeline.optimizer, 'param_groups')
        
        assert isinstance(pipeline.criterion, nn.BCEWithLogitsLoss)
    
    @pytest.mark.asyncio
    async def test_training_pipeline_train_epoch(self, training_config, sample_dataset):
        """Test training pipeline epoch training"""
        pipeline = TrainingPipeline(training_config)
        
        from torch.utils.data import DataLoader
        train_loader = DataLoader(sample_dataset, batch_size=4, shuffle=True)
        
        epoch_metrics = await pipeline._train_epoch(train_loader, epoch=1)
        
        assert 'loss' in epoch_metrics
        assert 'accuracy' in epoch_metrics
        assert 'precision' in epoch_metrics
        assert 'recall' in epoch_metrics
        assert 'f1' in epoch_metrics
        
        assert epoch_metrics['loss'] > 0
        assert 0 <= epoch_metrics['accuracy'] <= 1
        assert 0 <= epoch_metrics['precision'] <= 1
        assert 0 <= epoch_metrics['recall'] <= 1
        assert 0 <= epoch_metrics['f1'] <= 1
    
    @pytest.mark.asyncio
    async def test_training_pipeline_validate_epoch(self, training_config, sample_dataset):
        """Test training pipeline epoch validation"""
        pipeline = TrainingPipeline(training_config)
        
        from torch.utils.data import DataLoader
        val_loader = DataLoader(sample_dataset, batch_size=4, shuffle=False)
        
        val_metrics = await pipeline._validate_epoch(val_loader, epoch=1)
        
        assert 'val_loss' in val_metrics
        assert 'val_accuracy' in val_metrics
        assert 'val_precision' in val_metrics
        assert 'val_recall' in val_metrics
        assert 'val_f1' in val_metrics
        
        assert val_metrics['val_loss'] > 0
        assert 0 <= val_metrics['val_accuracy'] <= 1
        assert 0 <= val_metrics['val_precision'] <= 1
        assert 0 <= val_metrics['val_recall'] <= 1
        assert 0 <= val_metrics['val_f1'] <= 1
    
    def test_curriculum_learning_setup(self, training_config):
        """Test curriculum learning setup"""
        pipeline = TrainingPipeline(training_config)
        
        difficulty_1 = pipeline._get_curriculum_difficulty(epoch=1)
        difficulty_2 = pipeline._get_curriculum_difficulty(epoch=2)
        difficulty_3 = pipeline._get_curriculum_difficulty(epoch=3)
        
        assert difficulty_1 == 0.5  # First curriculum stage
        assert difficulty_2 == 1.0  # Second curriculum stage
        assert difficulty_3 == 1.0  # After curriculum, full difficulty
    
    def test_model_checkpointing(self, training_config):
        """Test model checkpointing functionality"""
        import tempfile
        import os
        
        with tempfile.TemporaryDirectory() as temp_dir:
            training_config.checkpoint_dir = temp_dir
            pipeline = TrainingPipeline(training_config)
            
            checkpoint_path = pipeline._save_checkpoint(
                epoch=1,
                metrics={'val_loss': 0.5, 'val_accuracy': 0.8},
                is_best=True
            )
            
            assert os.path.exists(checkpoint_path)
            
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            assert 'epoch' in checkpoint
            assert 'model_state_dict' in checkpoint
            assert 'optimizer_state_dict' in checkpoint
            assert 'scheduler_state_dict' in checkpoint
            assert 'metrics' in checkpoint
            
            assert checkpoint['epoch'] == 1
            assert checkpoint['metrics']['val_loss'] == 0.5
            assert checkpoint['metrics']['val_accuracy'] == 0.8
    
    def test_performance_metrics_calculation(self, training_config):
        """Test performance metrics calculation"""
        pipeline = TrainingPipeline(training_config)
        
        batch_size = 8
        num_classes = 71
        
        predictions = torch.randn(batch_size, num_classes)
        targets = torch.randint(0, 2, (batch_size, num_classes)).float()
        
        metrics = pipeline._calculate_metrics(predictions, targets)
        
        assert 'accuracy' in metrics
        assert 'precision' in metrics
        assert 'recall' in metrics
        assert 'f1' in metrics
        
        assert 0 <= metrics['accuracy'] <= 1
        assert 0 <= metrics['precision'] <= 1
        assert 0 <= metrics['recall'] <= 1
        assert 0 <= metrics['f1'] <= 1
    
    def test_early_stopping_mechanism(self, training_config):
        """Test early stopping mechanism"""
        pipeline = TrainingPipeline(training_config)
        
        val_losses = [1.0, 0.9, 0.85, 0.84, 0.84, 0.84, 0.84]  # No improvement after epoch 3
        
        should_stop = False
        for epoch, val_loss in enumerate(val_losses, 1):
            should_stop = pipeline._check_early_stopping(val_loss, epoch)
            if should_stop:
                break
        
        assert should_stop == True
        assert epoch >= training_config.early_stopping_patience
    
    @pytest.mark.asyncio
    async def test_training_pipeline_accuracy_improvement(self, training_config, sample_dataset):
        """Test that training pipeline achieves accuracy improvement target (2-5%)"""
        pipeline = TrainingPipeline(training_config)
        
        from torch.utils.data import DataLoader, random_split
        
        train_size = int(0.8 * len(sample_dataset))
        val_size = len(sample_dataset) - train_size
        train_dataset, val_dataset = random_split(sample_dataset, [train_size, val_size])
        
        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
        
        initial_metrics = await pipeline._validate_epoch(val_loader, epoch=0)
        initial_accuracy = initial_metrics['val_accuracy']
        
        for epoch in range(1, 3):  # Train for 2 epochs
            await pipeline._train_epoch(train_loader, epoch=epoch)
        
        final_metrics = await pipeline._validate_epoch(val_loader, epoch=2)
        final_accuracy = final_metrics['val_accuracy']
        
        accuracy_improvement = final_accuracy - initial_accuracy
        
        assert accuracy_improvement >= -0.1, f"Accuracy decreased significantly: {accuracy_improvement:.3f}"
    
    def test_ensemble_confidence_calibration(self, training_config):
        """Test ensemble confidence calibration"""
        pipeline = TrainingPipeline(training_config)
        
        batch_size = 16
        num_classes = 71
        
        cnn_logits = torch.randn(batch_size, num_classes)
        lstm_logits = torch.randn(batch_size, num_classes)
        transformer_logits = torch.randn(batch_size, num_classes)
        
        calibrated_probs = pipeline._calibrate_ensemble_confidence(
            cnn_logits, lstm_logits, transformer_logits
        )
        
        assert calibrated_probs.shape == (batch_size, num_classes)
        assert torch.all(calibrated_probs >= 0)
        assert torch.all(calibrated_probs <= 1)
        
        assert torch.all(calibrated_probs >= 0)
        assert torch.all(calibrated_probs <= 1)


class TestIntegratedDatasetFunctionality:
    """Test integration with public ECG datasets"""
    
    @pytest.fixture
    def mock_dataset_service(self):
        """Create mock dataset service for testing"""
        with patch('app.services.dataset_service.DatasetService') as mock_service:
            mock_instance = Mock()
            mock_service.return_value = mock_instance
            
            mock_instance.load_ptb_xl.return_value = {
                'signals': torch.randn(100, 12, 5000),
                'labels': torch.randint(0, 2, (100, 71)).float(),
                'metadata': {'sampling_rate': 500, 'dataset': 'PTB-XL'}
            }
            
            mock_instance.load_mit_bih.return_value = {
                'signals': torch.randn(50, 12, 5000),
                'labels': torch.randint(0, 2, (50, 71)).float(),
                'metadata': {'sampling_rate': 360, 'dataset': 'MIT-BIH'}
            }
            
            yield mock_instance
    
    @pytest.mark.asyncio
    async def test_dataset_integration_with_training(self, training_config, mock_dataset_service):
        """Test dataset integration with training pipeline"""
        ptb_data = mock_dataset_service.load_ptb_xl()
        mit_data = mock_dataset_service.load_mit_bih()
        
        combined_signals = torch.cat([ptb_data['signals'], mit_data['signals']], dim=0)
        combined_labels = torch.cat([ptb_data['labels'], mit_data['labels']], dim=0)
        
        dataset = ECGMultimodalDataset(
            signals=combined_signals,
            labels=combined_labels,
            sampling_rate=500,
            augment=False
        )
        
        pipeline = TrainingPipeline(training_config)
        
        from torch.utils.data import DataLoader
        data_loader = DataLoader(dataset, batch_size=4, shuffle=True)
        
        epoch_metrics = await pipeline._train_epoch(data_loader, epoch=1)
        
        assert 'loss' in epoch_metrics
        assert 'accuracy' in epoch_metrics
        assert epoch_metrics['loss'] > 0
    
    def test_dataset_compatibility_different_sampling_rates(self, mock_dataset_service):
        """Test handling of datasets with different sampling rates"""
        ptb_data = mock_dataset_service.load_ptb_xl()  # 500 Hz
        mit_data = mock_dataset_service.load_mit_bih()  # 360 Hz
        
        ptb_dataset = ECGMultimodalDataset(
            signals=ptb_data['signals'],
            labels=ptb_data['labels'],
            sampling_rate=500,
            augment=False
        )
        
        mit_dataset = ECGMultimodalDataset(
            signals=mit_data['signals'],
            labels=mit_data['labels'],
            sampling_rate=360,
            augment=False
        )
        
        ptb_sample = ptb_dataset[0]
        mit_sample = mit_dataset[0]
        
        assert ptb_sample['signal'].shape == (12, 5000)
        assert mit_sample['signal'].shape == (12, 5000)  # Should be resampled
        assert ptb_sample['metadata']['sampling_rate'] == 500
        assert mit_sample['metadata']['sampling_rate'] == 360
    
    def test_model_performance_with_integrated_datasets(self, model_config, mock_dataset_service):
        """Test model performance with integrated datasets"""
        ptb_data = mock_dataset_service.load_ptb_xl()
        
        model = HybridECGModel(model_config)
        model.eval()
        
        with torch.no_grad():
            output = model(ptb_data['signals'][:4])  # Test with 4 samples
        
        assert 'final_logits' in output
        assert output['final_logits'].shape == (4, 71)
        assert torch.all(torch.isfinite(output['final_logits']))
        
        predictions = torch.sigmoid(output['final_logits'])
        targets = ptb_data['labels'][:4]
        
        pred_binary = (predictions > 0.5).float()
        accuracy = (pred_binary == targets).float().mean()
        
        assert 0.3 <= accuracy <= 0.7, f"Accuracy {accuracy:.3f} seems unreasonable for random model"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
